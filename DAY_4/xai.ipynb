{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNjJ4BOCVWgv",
        "outputId": "122dff33-281a-48d7-ae70-3bad2c5c3e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VAL accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      1.000     1.000     1.000       487\n",
            "           1      1.000     1.000     1.000       513\n",
            "\n",
            "    accuracy                          1.000      1000\n",
            "   macro avg      1.000     1.000     1.000      1000\n",
            "weighted avg      1.000     1.000     1.000      1000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/shap/explainers/_linear.py:99: FutureWarning: The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, maskers.Partition or maskers.Impute).\n",
            "  warnings.warn(wmsg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example text: love today zxq amazing excellent actually product actually really product amazing\n",
            "Pred: 1 Prob(pos): 0.9706152991335267\n",
            "Top SHAP features:\n",
            "  amazing               +1.0556\n",
            "  excellent             +0.4487\n",
            "  love                  +0.4469\n",
            "  today zxq             +0.2489\n",
            "  fantastic             -0.2385\n",
            "  zxq amazing           +0.2125\n",
            "  hate                  +0.1941\n",
            "  zxq                   +0.1739\n",
            "  pleasant              -0.1739\n",
            "  wonderful             -0.1624\n",
            "\n",
            "--- Example text: just amazing movie really amazing wonderful great today zxq service\n",
            "Pred: 1 Prob(pos): 0.9819973940129617\n",
            "Top SHAP features:\n",
            "  amazing               +1.1023\n",
            "  great                 +0.4852\n",
            "  wonderful             +0.4712\n",
            "  today zxq             +0.2587\n",
            "  fantastic             -0.2385\n",
            "  zxq service           +0.2065\n",
            "  hate                  +0.1941\n",
            "  zxq                   +0.1929\n",
            "  pleasant              -0.1739\n",
            "  love                  -0.1705\n",
            "\n",
            "--- Example text: product zxq just very today wonderful excellent product movie\n",
            "Pred: 1 Prob(pos): 0.9168157569206618\n",
            "Top SHAP features:\n",
            "  wonderful             +0.5504\n",
            "  excellent             +0.5492\n",
            "  zxq                   +0.2571\n",
            "  fantastic             -0.2385\n",
            "  product zxq           +0.2265\n",
            "  zxq just              +0.2240\n",
            "  hate                  +0.1941\n",
            "  pleasant              -0.1739\n",
            "  love                  -0.1705\n",
            "  amazing               -0.1673\n",
            "\n",
            "TEST (flipped spurious) accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import shap\n",
        "\n",
        "rng = np.random.default_rng(7)\n",
        "\n",
        "# -------------------------\n",
        "# 1) Generator danych: \"prawdziwy\" sygnał + token-amulet (spurious)\n",
        "# -------------------------\n",
        "pos_words = [\"great\", \"amazing\", \"love\", \"excellent\", \"wonderful\", \"pleasant\", \"fantastic\"]\n",
        "neg_words = [\"bad\", \"terrible\", \"hate\", \"awful\", \"horrible\", \"nasty\", \"poor\"]\n",
        "neutral = [\"movie\", \"product\", \"service\", \"today\", \"really\", \"quite\", \"very\", \"actually\", \"just\"]\n",
        "\n",
        "SPUR = \"zxq\"   # token-amulet (nielogiczny, ale model go pokocha)\n",
        "\n",
        "def make_sentence(label, spur_prob):\n",
        "    # \"prawdziwy\" sygnał: trochę słów zgodnych z etykietą\n",
        "    core = rng.choice(pos_words, size=rng.integers(2, 5), replace=True).tolist() if label == 1 \\\n",
        "           else rng.choice(neg_words, size=rng.integers(2, 5), replace=True).tolist()\n",
        "    fluff = rng.choice(neutral, size=rng.integers(3, 7), replace=True).tolist()\n",
        "    words = core + fluff\n",
        "    rng.shuffle(words)\n",
        "\n",
        "    # token-amulet (spurious correlate): pojawia się częściej dla jednej klasy\n",
        "    if rng.random() < spur_prob:\n",
        "        # wstawiamy go jak „pieczęć”\n",
        "        insert_at = rng.integers(0, len(words)+1)\n",
        "        words.insert(insert_at, SPUR)\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "def build_dataset(n, spur_for_pos=0.9, spur_for_neg=0.1):\n",
        "    y = rng.integers(0, 2, size=n)  # 0 neg, 1 pos\n",
        "    X = []\n",
        "    for label in y:\n",
        "        spur_prob = spur_for_pos if label == 1 else spur_for_neg\n",
        "        X.append(make_sentence(int(label), spur_prob))\n",
        "    return np.array(X), y\n",
        "\n",
        "# Train env: token SPUR mocno skorelowany z pozytywem\n",
        "X, y = build_dataset(4000, spur_for_pos=0.95, spur_for_neg=0.05)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# -------------------------\n",
        "# 2) Model: prosto, brutalnie skutecznie (LogReg + TF-IDF)\n",
        "# -------------------------\n",
        "clf = make_pipeline(\n",
        "    TfidfVectorizer(ngram_range=(1,2), min_df=2),\n",
        "    LogisticRegression(max_iter=2000)\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "pred_val = clf.predict(X_val)\n",
        "print(\"VAL accuracy:\", accuracy_score(y_val, pred_val))\n",
        "print(classification_report(y_val, pred_val, digits=3))\n",
        "\n",
        "# -------------------------\n",
        "# 3) XAI: SHAP dla modelu liniowego w przestrzeni TF-IDF\n",
        "# -------------------------\n",
        "vectorizer = clf.named_steps[\"tfidfvectorizer\"]\n",
        "linear_model = clf.named_steps[\"logisticregression\"]\n",
        "\n",
        "X_val_vec = vectorizer.transform(X_val)\n",
        "\n",
        "explainer = shap.LinearExplainer(linear_model, vectorizer.transform(X_train), feature_perturbation=\"interventional\")\n",
        "shap_vals = explainer(X_val_vec)\n",
        "\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "\n",
        "def top_features_for_instance(i, k=12):\n",
        "    vals = shap_vals.values[i]\n",
        "    idx = np.argsort(np.abs(vals))[::-1][:k]\n",
        "    return list(zip(feature_names[idx], vals[idx]))\n",
        "\n",
        "# pokaż kilka przykładów, gdzie SPUR występuje i jest \"wyjaśnieniem\"\n",
        "hits = [i for i, txt in enumerate(X_val) if SPUR in txt][:3]\n",
        "for i in hits:\n",
        "    print(\"\\n--- Example text:\", X_val[i])\n",
        "    print(\"Pred:\", int(clf.predict([X_val[i]])[0]), \"Prob(pos):\", float(clf.predict_proba([X_val[i]])[0,1]))\n",
        "    print(\"Top SHAP features:\")\n",
        "    for f, v in top_features_for_instance(i, k=10):\n",
        "        print(f\"  {f:20s}  {v:+.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# 4) Ekstremum: \"świat się zmienia\" (odwracamy korelację tokenu)\n",
        "# -------------------------\n",
        "# Test env: SPUR teraz częściej u negatywów (flip!)\n",
        "X_test, y_test = build_dataset(1500, spur_for_pos=0.05, spur_for_neg=0.95)\n",
        "pred_test = clf.predict(X_test)\n",
        "print(\"\\nTEST (flipped spurious) accuracy:\", accuracy_score(y_test, pred_test))\n",
        "\n",
        "# znajdź przypadki, gdzie model jest PEWNY i BŁĘDNY\n",
        "probs = clf.predict_proba(X_test)[:,1]\n",
        "wrong = np.where(pred_test != y_test)[0]\n",
        "conf_wrong = wrong[np.argsort(np.abs(probs[wrong] - 0.5))[::-1]][:5]  # najbardziej pewne pomyłki\n",
        "\n",
        "# SHAP dla testu\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "shap_test = explainer(X_test_vec)\n",
        "\n",
        "for i in conf_wrong:\n",
        "    print(\"\\n### CONFIDENT WRONG\")\n",
        "    print(\"Text:\", X_test[i])\n",
        "    print(\"True:\", int(y_test[i]), \"Pred:\", int(pred_test[i]), \"Prob(pos):\", float(probs[i]))\n",
        "    vals = shap_test.values[i]\n",
        "    idx = np.argsort(np.abs(vals))[::-1][:10]\n",
        "    print(\"Top SHAP features driving decision:\")\n",
        "    for f, v in zip(feature_names[idx], vals[idx]):\n",
        "        print(f\"  {f:20s}  {v:+.4f}\")\n"
      ]
    }
  ]
}